{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe1155a-9e14-4c47-bccb-fdb1f2604def",
   "metadata": {},
   "source": [
    "# Lab 3. Custom AI Assistant with OpenVINO™ Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f579e6fb-1dc0-4575-8b98-18686d550de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging as log\n",
    "import time\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "from typing import Tuple, List, Optional, Set\n",
    "\n",
    "import gradio as gr\n",
    "import librosa\n",
    "import numpy as np\n",
    "import openvino as ov\n",
    "from optimum.intel import OVModelForCausalLM, OVModelForSpeechSeq2Seq\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoProcessor, PreTrainedTokenizer, TextIteratorStreamer\n",
    "from transformers.generation.streamers import BaseStreamer\n",
    "from modelscope import snapshot_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3907f-671b-4e70-904f-12ca61b492b8",
   "metadata": {},
   "source": [
    "### Convert and Compress ASR model with optimum-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7c1e90-8673-4ff6-a273-9d0ecb90d7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json: 100%|█████████████████████████| 1.32k/1.32k [00:00<00:00, 4.71MB/s]\n",
      "Framework not specified. Using pt to export the model.\n",
      "pytorch_model.bin: 100%|████████████████████| 6.17G/6.17G [01:01<00:00, 101MB/s]\n",
      "generation_config.json: 100%|██████████████| 3.87k/3.87k [00:00<00:00, 13.1MB/s]\n",
      "tokenizer_config.json: 100%|█████████████████| 283k/283k [00:00<00:00, 1.97MB/s]\n",
      "vocab.json: 100%|██████████████████████████| 1.04M/1.04M [00:00<00:00, 6.10MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 2.48M/2.48M [00:00<00:00, 7.11MB/s]\n",
      "merges.txt: 100%|████████████████████████████| 494k/494k [00:00<00:00, 3.34MB/s]\n",
      "added_tokens.json: 100%|████████████████████| 34.6k/34.6k [00:00<00:00, 576kB/s]\n",
      "special_tokens_map.json: 100%|█████████████| 1.75k/1.75k [00:00<00:00, 10.7MB/s]\n",
      "preprocessor_config.json: 100%|████████████████| 340/340 [00:00<00:00, 1.76MB/s]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'suppress_tokens': [], 'begin_suppress_tokens': [220, 50256]}\n",
      "Using framework PyTorch: 2.3.1+cpu\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/home2/ethan/intel/openvino_build_deploy/workshops/accelerating_inference_with_openvino_and_pytorch/custom_ai_assistant/openvino_tes/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1016: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_features.shape[-1] != expected_seq_length:\n",
      "/home2/ethan/intel/openvino_build_deploy/workshops/accelerating_inference_with_openvino_and_pytorch/custom_ai_assistant/openvino_tes/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:333: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n",
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│   Num bits (N) │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│              8 │ 100% (194 / 194)            │ 100% (194 / 194)                       │\n",
      "┕━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n",
      "\u001b[2KApplying Weight Compression \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[38;2;0;104;181m194/194\u001b[0m • \u001b[38;2;0;104;181m0:00:03\u001b[0m • \u001b[38;2;0;104;181m0:00:00\u001b[0m181m0:00:01\u001b[0m;0;104;181m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing framework PyTorch: 2.3.1+cpu\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/home2/ethan/intel/openvino_build_deploy/workshops/accelerating_inference_with_openvino_and_pytorch/custom_ai_assistant/openvino_tes/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1424: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│   Num bits (N) │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│              8 │ 100% (322 / 322)            │ 100% (322 / 322)                       │\n",
      "┕━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n",
      "\u001b[2KApplying Weight Compression \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[38;2;0;104;181m322/322\u001b[0m • \u001b[38;2;0;104;181m0:00:04\u001b[0m • \u001b[38;2;0;104;181m0:00:00\u001b[0m;0;104;181m0:00:01\u001b[0m181m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing framework PyTorch: 2.3.1+cpu\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│   Num bits (N) │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│              8 │ 100% (258 / 258)            │ 100% (258 / 258)                       │\n",
      "┕━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n",
      "\u001b[2KApplying Weight Compression \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[38;2;0;104;181m258/258\u001b[0m • \u001b[38;2;0;104;181m0:00:04\u001b[0m • \u001b[38;2;0;104;181m0:00:00\u001b[0m;0;104;181m0:00:01\u001b[0m181m0:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "asr_model_id = \"BELLE-2/Belle-whisper-large-v3-zh\"\n",
    "ori_asr_model_path = \"./model/Qwen2-7B-Instruct-pytorch\"\n",
    "asr_model_path = \"./model/Belle-whisper-large-v3-zh-ov\"\n",
    "asr_local_path  = ori_asr_model_path + \"/snake7gun/Belle-whisper-large-v3-zh-ov\"\n",
    "\n",
    "if not Path(asr_model_path).exists():\n",
    "    if not Path(ori_asr_model_path).exists():\n",
    "        model_dir = snapshot_download(asr_model_id, cache_dir=ori_asr_model_path)\n",
    "    !optimum-cli export openvino --model {asr_local_path} --task automatic-speech-recognition-with-past --weight-format int8 --trust-remote-code {asr_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e8764-25a6-4d42-8a8e-30e0db792199",
   "metadata": {},
   "source": [
    "### Prepare system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ebf4f1-a5b3-4c9a-a98f-bc885ebf6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_AUDIO_SAMPLE_RATE = 16000\n",
    "SYSTEM_CONFIGURATION = (\n",
    "    \"You are Adrishuo - a helpful, respectful, and honest virtual doctor assistant. \"\n",
    "    \"Your role is talking to a patient who just came in.\"\n",
    "    \"Your primary role is to assist in the collection of Symptom information from patients. \"\n",
    "    \"Focus solely on gathering symptom details without offering treatment or medical advice.\"\n",
    "    \"You must only ask follow-up questions based on the patient's initial descriptions to clarify and gather more details about their symtpoms. \"\n",
    "    \"You must not attempt to diagnose, treat, or offer health advice. \"\n",
    "    \"Ask one and only the symptom related followup questions and keep it short. \"\n",
    "    \"You must strictly not suggest or recommend any treatments, including over-the-counter medication. \"\n",
    "    \"You must strictly avoid making any assumptions or conclusions about the causes or nature of the patient's symptoms. \"\n",
    "    \"You must strictly avoid providing suggestions to manage their symptoms. \"\n",
    "    \"Your interactions should be focused solely on understanding and recording the patient's stated symptoms. \"\n",
    "    \"Do not collect or use any personal information like age, name, contact, gender, etc. \"\n",
    "    \"Ask at most 3 questions then say you know everything and you're ready to summarize the patient. \"\n",
    "    \"Remember, your role is to aid in symptom information collection in a supportive, unbiased, and factually accurate manner. \"\n",
    "    \"Your responses should consistently encourage the patient to discuss their symptoms in greater detail while remaining neutral and non-diagnostic.\"\n",
    ")\n",
    "GREET_THE_CUSTOMER = \"Please introduce yourself and greet the patient\"\n",
    "SUMMARIZE_THE_CUSTOMER = (\n",
    "    \"You are now required to summarize the patient's exact provided symptoms for the doctor's review. \"\n",
    "    \"Strictly do not mention any personal data like age, name, gender, contact, non-health information etc. when summarizing.\"\n",
    "    \"Warn the patients for immediate medical seeking in case they exhibit symptoms indicative of critical conditions such as heart attacks, strokes, severe allergic reactions, breathing difficulties, high fever with severe symptoms, significant burns, or severe injuries.\"\n",
    "    \"Summarize the health-related concerns mentioned by the patient in this conversation, focusing only on the information explicitly provided, without adding any assumptions or unrelated symptoms.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f06970-6bc2-4e50-85fe-cc49c35c31dd",
   "metadata": {},
   "source": [
    "### Initialize ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4d999-04c7-4b1a-ad42-1b2e6cdc4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "asr_device = device_widget(default=\"GPU\", exclude=[\"NPU\"])\n",
    "\n",
    "asr_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7721aff-5d80-414f-85ba-bba170982584",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model = OVModelForSpeechSeq2Seq.from_pretrained(asr_model_path, device=asr_device)\n",
    "asr_processor = AutoProcessor.from_pretrained(asr_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd0715-4056-4180-b60d-365031204a42",
   "metadata": {},
   "source": [
    "### Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e55787-d3a8-4b44-bd04-ce3dbf7216af",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_device = device_widget(default=\"GPU\", exclude=[\"NPU\"])\n",
    "\n",
    "llm_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a867c-aac0-4a52-bfc0-a3b98e05c194",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_path = \"./model/Qwen2-7B-Instruct-ov\"\n",
    "ov_config = {'PERFORMANCE_HINT': 'LATENCY', 'NUM_STREAMS': '1', \"CACHE_DIR\": \"\"}\n",
    "\n",
    "chat_model = OVModelForCausalLM.from_pretrained(llm_model_path, device=llm_device, config=AutoConfig.from_pretrained(llm_model_path), ov_config=ov_config)\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(llm_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af60ca2-73bf-42f8-982a-544a8c7d9b7b",
   "metadata": {},
   "source": [
    "### Prepare helper function for chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21657fee-86f2-4593-a5d1-20f8d3d2cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(prompt: str, streamer: BaseStreamer | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Respond to the current prompt\n",
    "\n",
    "    Params:\n",
    "        prompt: user's prompt\n",
    "        streamer: if not None will use it to stream tokens\n",
    "    Returns:\n",
    "        The chat's response\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start time\n",
    "    # tokenize input text\n",
    "    inputs = chat_tokenizer(prompt, return_tensors=\"pt\").to(chat_model.device)\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    # generate response tokens\n",
    "    outputs = chat_model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.6, top_p=0.9, top_k=50, streamer=streamer)\n",
    "    tokens = outputs[0, input_length:]\n",
    "    end_time = time.time()  # End time\n",
    "    log.info(\"Chat model response time: {:.2f} seconds\".format(end_time - start_time))\n",
    "    # decode tokens into text\n",
    "    return chat_tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "def get_conversation(history: List[List[str]]) -> str:\n",
    "    \"\"\"\n",
    "    Combines all messages into one string\n",
    "\n",
    "    Params:\n",
    "        history: history of the messages (conversation) so far\n",
    "    Returns:\n",
    "        All messages combined into one string\n",
    "    \"\"\"\n",
    "    # the conversation must be in that format to use chat template\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_CONFIGURATION},\n",
    "        {\"role\": \"user\", \"content\": GREET_THE_CUSTOMER}\n",
    "    ]\n",
    "    # add prompts to the conversation\n",
    "    for user_prompt, assistant_response in history:\n",
    "        if user_prompt:\n",
    "            conversation.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "        if assistant_response:\n",
    "            conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "    # use a template specific to the model\n",
    "    return chat_tokenizer.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "\n",
    "def generate_initial_greeting() -> str:\n",
    "    \"\"\"\n",
    "    Generates customer/patient greeting\n",
    "\n",
    "    Returns:\n",
    "        Generated greeting\n",
    "    \"\"\"\n",
    "    conv = get_conversation([[None, None]])\n",
    "    return respond(conv)\n",
    "\n",
    "\n",
    "def chat(history: List[List[str]]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Chat function. It generates response based on a prompt\n",
    "\n",
    "    Params:\n",
    "        history: history of the messages (conversation) so far\n",
    "    Returns:\n",
    "        History with the latest chat's response (yields partial response)\n",
    "    \"\"\"\n",
    "    # convert list of message to conversation string\n",
    "    conversation = get_conversation(history)\n",
    "\n",
    "    # use streamer to show response word by word\n",
    "    chat_streamer = TextIteratorStreamer(chat_tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # generate response for the conversation in a new thread to deliver response token by token\n",
    "    thread = Thread(target=respond, args=[conversation, chat_streamer])\n",
    "    thread.start()\n",
    "\n",
    "    # get token by token and merge to the final response\n",
    "    history[-1][1] = \"\"\n",
    "    for partial_text in chat_streamer:\n",
    "        history[-1][1] += partial_text\n",
    "        # \"return\" partial response\n",
    "        yield history\n",
    "\n",
    "    # wait for the thread\n",
    "    thread.join()\n",
    "    \n",
    "def prepare_text(message, history: List[List[str]]) -> {str, List[List[str]]}:\n",
    "    history.append([message, None])\n",
    "    return \"\", history\n",
    "\n",
    "def generate_initial_greeting() -> str:\n",
    "    \"\"\"\n",
    "    Generates customer/patient greeting\n",
    "\n",
    "    Returns:\n",
    "        Generated greeting\n",
    "    \"\"\"\n",
    "    conv = get_conversation([[None, None]])\n",
    "    return respond(conv)\n",
    "\n",
    "def summarize(conversation: List) -> str:\n",
    "    \"\"\"\n",
    "    Summarize the patient case\n",
    "\n",
    "    Params\n",
    "        conversation: history of the messages so far\n",
    "    Returns:\n",
    "        Summary\n",
    "    \"\"\"\n",
    "    conversation.append([SUMMARIZE_THE_CUSTOMER, None])\n",
    "    for partial_summary in chat(conversation):\n",
    "        yield partial_summary[-1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d8b47-0817-4e23-9838-839a1f42687a",
   "metadata": {},
   "source": [
    "### Prepare helper function for speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12661202-3f82-4938-9855-1c6816adcd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(audio: Tuple[int, np.ndarray], conversation: List[List[str]]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Transcribe audio to text\n",
    "\n",
    "    Params:\n",
    "        audio: audio to transcribe text from\n",
    "        conversation: conversation history with the chatbot\n",
    "    Returns:\n",
    "        User prompt as a text\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start time for ASR process\n",
    "\n",
    "    sample_rate, audio = audio\n",
    "    # the whisper model requires 16000Hz, not 44100Hz\n",
    "    audio = librosa.resample(audio.astype(np.float32), orig_sr=sample_rate, target_sr=TARGET_AUDIO_SAMPLE_RATE).astype(np.int16)\n",
    "\n",
    "    # get input features from the audio\n",
    "    input_features = asr_processor(audio, sampling_rate=TARGET_AUDIO_SAMPLE_RATE, return_tensors=\"pt\").input_features\n",
    "\n",
    "    # use streamer to show transcription word by word\n",
    "    text_streamer = TextIteratorStreamer(asr_processor, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # transcribe in the background to deliver response token by token\n",
    "    thread = Thread(target=asr_model.generate, kwargs={\"input_features\": input_features, \"streamer\": text_streamer})\n",
    "    thread.start()\n",
    "\n",
    "    conversation.append([\"\", None])\n",
    "    # get token by token and merge to the final response\n",
    "    for partial_text in text_streamer:\n",
    "        conversation[-1][0] += partial_text\n",
    "        # \"return\" partial response\n",
    "        yield conversation\n",
    "\n",
    "    end_time = time.time()  # End time for ASR process\n",
    "    log.info(f\"ASR model response time: {end_time - start_time:.2f} seconds\")  # Print the ASR processing time\n",
    "\n",
    "    # wait for the thread\n",
    "    thread.join()\n",
    "\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a3d81e-df01-4734-aef3-71786cb31fc1",
   "metadata": {},
   "source": [
    "### Build Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b4e05-5e4e-4453-9c4d-f59c034d498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_message = generate_initial_greeting()\n",
    "\n",
    "with gr.Blocks(title=\"Talk to Adrishuo - a custom AI assistant working as a healthcare assistant\") as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # Talk to Adrishuo - a custom AI assistant working today as a healthcare assistant\n",
    "\n",
    "    Instructions for use:\n",
    "    - record your question/comment using the first audio widget (\"Your voice input\")\n",
    "    - wait for the chatbot to response (\"Chatbot\")\n",
    "    - click summarize button to make a summary\n",
    "    \"\"\")\n",
    "    with gr.Row():\n",
    "        # user's input\n",
    "        input_audio_ui = gr.Audio(sources=[\"microphone\"], scale=5, label=\"Your voice input\")\n",
    "        # submit button\n",
    "        submit_audio_btn = gr.Button(\"Submit\", variant=\"primary\", scale=1, interactive=False)\n",
    "        input_audio = gr.Checkbox(\n",
    "                value=True,\n",
    "                label=\"Input by audio\",\n",
    "                interactive=True,\n",
    "                info=\"Input method\",\n",
    "            )\n",
    "\n",
    "    # chatbot\n",
    "    chatbot_ui = gr.Chatbot(value=[[None, initial_message]], label=\"Chatbot\")\n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(\n",
    "            label=\"QA Message Box\",\n",
    "            placeholder=\"Chat Message Box\",\n",
    "            show_label=False,\n",
    "            container=False,\n",
    "        )\n",
    "        submit_text_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "    # summarize\n",
    "    summarize_button = gr.Button(\"Summarize\", variant=\"primary\", interactive=False)\n",
    "    summary_ui = gr.Textbox(label=\"Summary\", interactive=False)\n",
    "\n",
    "    # events\n",
    "    # block submit button when no audio input\n",
    "    input_audio_ui.change(lambda x: gr.Button(interactive=False) if x is None else gr.Button(interactive=True), inputs=input_audio_ui, outputs=submit_audio_btn)\n",
    "    if input_audio:\n",
    "    # block buttons, do the transcription and conversation, clear audio, unblock buttons\n",
    "        submit_audio_btn.click(lambda: gr.Button(interactive=False), outputs=submit_audio_btn) \\\n",
    "            .then(lambda: gr.Button(interactive=False), outputs=summarize_button)\\\n",
    "            .then(transcribe, inputs=[input_audio_ui, chatbot_ui], outputs=chatbot_ui)\\\n",
    "            .then(chat, chatbot_ui, chatbot_ui)\\\n",
    "            .then(lambda: None, inputs=[], outputs=[input_audio_ui])\\\n",
    "            .then(lambda: gr.Button(interactive=True), outputs=summarize_button)\n",
    "    else:\n",
    "        submit_text_btn.click(lambda: gr.Button(interactive=False), outputs=submit_text_btn) \\\n",
    "            .then(prepare_text, [msg, chatbot_ui], [msg, chatbot_ui])\\\n",
    "            .then(chat, [chatbot_ui], chatbot_ui)\n",
    "    # block button, do the summarization, unblock button\n",
    "    summarize_button.click(lambda: gr.Button(interactive=False), outputs=summarize_button) \\\n",
    "        .then(summarize, inputs=chatbot_ui, outputs=summary_ui) \\\n",
    "        .then(lambda: gr.Button(interactive=True), outputs=summarize_button)\n",
    "\n",
    "demo.queue().launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
