{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe1155a-9e14-4c47-bccb-fdb1f2604def",
   "metadata": {},
   "source": [
    "# Lab 1. Text-completion with GenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2ea3b7-4e7b-4782-b35d-50fb4576ad8d",
   "metadata": {},
   "source": [
    "### Convert and Compress LLM model with optimum-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d35c93-6341-4422-a41f-ea084d597579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from modelscope import snapshot_download\n",
    "\n",
    "llm_model_id = \"qwen/Qwen2-7B-Instruct\"\n",
    "ori_llm_model_path = \"./model/Qwen2-7B-Instruct-pytorch\"\n",
    "llm_model_path = \"./model/Qwen2-7B-Instruct-ov\"\n",
    "llm_local_path  = ori_llm_model_path + \"/qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "if not Path(llm_model_path).exists():\n",
    "    if not Path(ori_llm_model_path).exists():\n",
    "        model_dir = snapshot_download(llm_model_id, cache_dir=ori_llm_model_path)\n",
    "\n",
    "    !optimum-cli export openvino --model {llm_local_path} --task text-generation-with-past --weight-format int4 --group-size 128 --ratio 0.8 --trust-remote-code {llm_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e8764-25a6-4d42-8a8e-30e0db792199",
   "metadata": {},
   "source": [
    "### Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ebf4f1-a5b3-4c9a-a98f-bc885ebf6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino_genai as ov_genai\n",
    "\n",
    "pipe = ov_genai.LLMPipeline(llm_model_path, \"GPU\")\n",
    "\n",
    "def streamer(subword):\n",
    "    print(subword, end='', flush=True)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f06970-6bc2-4e50-85fe-cc49c35c31dd",
   "metadata": {},
   "source": [
    "### Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ef4d999-04c7-4b1a-ad42-1b2e6cdc4bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVINO（Open Visual Inference and Neural Network Optimization）是由英特尔开发的一套开源工具套件，用于在各种设备上部署深度学习模型。它提供了一个API库和一组工具，用于优化神经网络模型的性能，并在不同的硬件平台上进行推理。\n",
      "\n",
      "OpenVINO的主要功能包括：\n",
      "\n",
      "1. **模型优化**：它能够优化神经网络模型，以适应不同的硬件平台，包括CPU、GPU、VPU（Vision Processing Unit）等。优化过程包括模型转换、量化、并行化等步骤，以提高模型的执行效率。\n",
      "\n",
      "2. **推理引擎**：OpenVINO提供了一个高性能的推理引擎，用于在各种硬件平台上执行模型推理。它支持多种硬件加速技术，如OpenCL、VPU等，以实现快速的实时推理。\n",
      "\n",
      "3. **API支持**：它提供了多种编程语言的API接口，如C++、Python等，方便开发者在不同的开发环境中使用。\n",
      "\n",
      "4. **模型库**：OpenVINO还包含了一些预训练的模型库，开发者可以直接使用这些模型进行特定任务的开发，而无需从头开始训练模型。\n",
      "\n",
      "5. **支持多种框架**：它支持多种深度学习框架，如TensorFlow、Caffe、ONNX等，使得开发者可以方便地将模型从这些框架转换到OpenVINO进行优化和部署。\n",
      "\n",
      "通过使用OpenVINO，开发者可以更高效地将深度学习模型部署到实际应用中，特别是在需要在边缘设备上进行实时推理的场景中。"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OpenVINO（Open Visual Inference and Neural Network Optimization）是由英特尔开发的一套开源工具套件，用于在各种设备上部署深度学习模型。它提供了一个API库和一组工具，用于优化神经网络模型的性能，并在不同的硬件平台上进行推理。\\n\\nOpenVINO的主要功能包括：\\n\\n1. **模型优化**：它能够优化神经网络模型，以适应不同的硬件平台，包括CPU、GPU、VPU（Vision Processing Unit）等。优化过程包括模型转换、量化、并行化等步骤，以提高模型的执行效率。\\n\\n2. **推理引擎**：OpenVINO提供了一个高性能的推理引擎，用于在各种硬件平台上执行模型推理。它支持多种硬件加速技术，如OpenCL、VPU等，以实现快速的实时推理。\\n\\n3. **API支持**：它提供了多种编程语言的API接口，如C++、Python等，方便开发者在不同的开发环境中使用。\\n\\n4. **模型库**：OpenVINO还包含了一些预训练的模型库，开发者可以直接使用这些模型进行特定任务的开发，而无需从头开始训练模型。\\n\\n5. **支持多种框架**：它支持多种深度学习框架，如TensorFlow、Caffe、ONNX等，使得开发者可以方便地将模型从这些框架转换到OpenVINO进行优化和部署。\\n\\n通过使用OpenVINO，开发者可以更高效地将深度学习模型部署到实际应用中，特别是在需要在边缘设备上进行实时推理的场景中。'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"<|im_start|>system\\n<|im_end|>\\n<|im_start|>user\\n什么是OpenVINO？<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "pipe.generate(prompt, eos_token_id=151645, max_length=500, streamer=streamer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
